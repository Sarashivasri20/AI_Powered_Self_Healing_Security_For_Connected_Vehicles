{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 3,
>>>>>>> c73354a330b159f9adab6290ae89bc89361d717a
   "id": "a001c11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "🔧 Loading DistilGPT2...\n",
=======
      "🔧 Loading DistilGPT2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saras\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saras\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
>>>>>>> c73354a330b159f9adab6290ae89bc89361d717a
      "🔧 Tokenizing dataset...\n"
     ]
    },
    {
<<<<<<< HEAD
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699e4e818dae45548e2bf072d3ca53e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krish\\AppData\\Local\\Temp\\ipykernel_53884\\2466626881.py:148: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 4:01:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.948400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.750800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.535800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.521200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.488200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.440300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.427100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.395900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.324600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.285400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.269600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.284600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.271100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.268700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.272200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.262100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.247900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.243300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.260900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.241300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.220800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.228900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuning complete!\n"
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 3194.53 examples/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 135\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m########################################\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Training configuration\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m########################################\u001b[39;00m\n\u001b[32m    133\u001b[39m base_path = \u001b[33m\"\u001b[39m\u001b[33m./\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Change this as needed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m training_args = \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel/distilgpt2_model\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m trainer = Trainer(\n\u001b[32m    149\u001b[39m     model=model,\n\u001b[32m    150\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m     data_collator=data_collator,\n\u001b[32m    154\u001b[39m )\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m########################################\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Train and Save\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;66;03m########################################\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:134\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, include_for_metrics, eval_do_concat_batches, fp16_backend, evaluation_strategy, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, eval_use_gather_object, average_tokens_across_devices)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saras\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1772\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1770\u001b[39m \u001b[38;5;66;03m# Initialize device before we proceed\u001b[39;00m\n\u001b[32m   1771\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.framework == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_torch_available():\n\u001b[32m-> \u001b[39m\u001b[32m1772\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m   1774\u001b[39m \u001b[38;5;66;03m# Disable average tokens when using single device\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.average_tokens_across_devices:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saras\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:2294\u001b[39m, in \u001b[36mTrainingArguments.device\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2290\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2291\u001b[39m \u001b[33;03mThe device used by this process.\u001b[39;00m\n\u001b[32m   2292\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2293\u001b[39m requires_backends(\u001b[38;5;28mself\u001b[39m, [\u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m2294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_setup_devices\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saras\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:62\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, objtype)\u001b[39m\n\u001b[32m     60\u001b[39m cached = \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     cached = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saras\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:2167\u001b[39m, in \u001b[36mTrainingArguments._setup_devices\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_accelerate_available():\n\u001b[32m-> \u001b[39m\u001b[32m2167\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2168\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the `Trainer` with `PyTorch` requires `accelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2169\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease run `pip install transformers[torch]` or `pip install \u001b[39m\u001b[33m'\u001b[39m\u001b[33maccelerate>=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mACCELERATE_MIN_VERSION\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2170\u001b[39m         )\n\u001b[32m   2171\u001b[39m \u001b[38;5;66;03m# We delay the init of `PartialState` to the end for clarity\u001b[39;00m\n\u001b[32m   2172\u001b[39m accelerator_state_kwargs = {\u001b[33m\"\u001b[39m\u001b[33menabled\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33muse_configured_state\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}\n",
      "\u001b[31mImportError\u001b[39m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>=0.26.0'`"
>>>>>>> c73354a330b159f9adab6290ae89bc89361d717a
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Dataset file paths\n",
    "dataset_files = [\n",
    "    \"../data/Attack_free_dataset.txt\",\n",
    "    \"../data/DoS_attack_dataset.txt\",\n",
    "    \"../data/Fuzzy_attack_dataset.txt\",\n",
    "    \"../data/Impersonation_attack_dataset.txt\"\n",
    "]\n",
    "\n",
    "########################################\n",
    "# Patch + Explanation logic\n",
    "########################################\n",
    "def get_patch_and_explanation_for_attack(attack_type, payload):\n",
    "    if attack_type == \"DoS\":\n",
    "        explanation = \"A DoS attack floods the CAN bus with high-priority messages, preventing normal communication.\"\n",
    "        patch = \"Activate the vehicle's security update to handle message overloads.\"\n",
    "    elif attack_type == \"Fuzzy\":\n",
    "        explanation = \"A Fuzzy attack sends malformed or random data to confuse or crash ECUs.\"\n",
    "        patch = \"Enable data validity checks to prevent unsafe reads.\"\n",
    "    elif attack_type == \"Impersonation\":\n",
    "        explanation = \"An Impersonation attack uses spoofed IDs to mimic legitimate ECUs and inject malicious data.\"\n",
    "        patch = \"Turn on ID verification to block unauthorized messages.\"\n",
    "    elif attack_type == \"Attack_free\":\n",
    "        explanation = \"No attack detected. All vehicle systems appear to be operating normally.\"\n",
    "        patch = \"No action needed – vehicle is normal.\"\n",
    "    else:\n",
    "        explanation = \"Unknown attack type.\"\n",
    "        patch = \"No patch suggestion available.\"\n",
    "\n",
    "    return f\"Attack Type: {attack_type}\\nExplanation: {explanation}\\nSuggested Patch: {patch}\"\n",
    "\n",
    "########################################\n",
    "# Load & Preprocess CAN data\n",
    "########################################\n",
    "def load_and_preprocess_data(dataset_paths):\n",
    "    data = []\n",
    "\n",
    "    for file_path in dataset_paths:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        attack_type = os.path.basename(file_path).split(\"_\")[0]\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 7:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                can_id_hex = parts[3]\n",
    "                can_id = int(can_id_hex, 16)\n",
    "                dlc = int(parts[6])\n",
    "                data_bytes = parts[7:7 + dlc]\n",
    "                byte_values = [int(b, 16) for b in data_bytes]\n",
    "\n",
    "                while len(byte_values) < 8:\n",
    "                    byte_values.append(0)\n",
    "\n",
    "                # ✅ Input prompt contains only CAN data\n",
    "                prompt = f\"CAN ID: {can_id}, DLC: {dlc}, Data: {byte_values}\"\n",
    "\n",
    "                # ✅ Target contains attack type, explanation, patch\n",
    "                response = get_patch_and_explanation_for_attack(attack_type, byte_values)\n",
    "\n",
    "                data.append({\"input_text\": prompt, \"target_text\": response})\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load and preprocess data\n",
    "data = load_and_preprocess_data(dataset_files)\n",
    "if not data:\n",
    "    print(\"⚠ No valid lines parsed. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Convert to Hugging Face dataset\n",
    "hf_dataset = Dataset.from_dict({\n",
    "    \"input_text\": [item[\"input_text\"] for item in data],\n",
    "    \"target_text\": [item[\"target_text\"] for item in data]\n",
    "})\n",
    "\n",
    "# Shuffle and use only the first 5000 samples\n",
    "hf_dataset = hf_dataset.shuffle(seed=42).select(range(min(5000, len(hf_dataset))))\n",
    "\n",
    "########################################\n",
    "# Load tokenizer and model\n",
    "########################################\n",
    "print(\"🔧 Loading DistilGPT2...\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Prevent padding errors\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "########################################\n",
    "# Tokenization\n",
    "########################################\n",
    "def tokenize_function(example):\n",
    "    input_ids = tokenizer(\n",
    "        example[\"input_text\"] + \"\\n\" + example[\"target_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    return input_ids\n",
    "\n",
    "print(\"🔧 Tokenizing dataset...\")\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=False, remove_columns=[\"input_text\", \"target_text\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "########################################\n",
    "# Training configuration\n",
    "########################################\n",
    "base_path = \"./\"  # Change this as needed\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(base_path, \"model/distilgpt2_model\"),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=os.path.join(base_path, \"logs\"),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "########################################\n",
    "# Train and Save\n",
    "########################################\n",
    "print(\"🚀 Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "save_path = os.path.join(base_path, \"model/fine_tuned_distilgpt2\")\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"✅ Fine-tuning complete!\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "id": "ad51b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Dataset file paths\n",
    "dataset_files = [\n",
    "    \"../data/Attack_free_dataset.txt\",\n",
    "    \"../data/DoS_attack_dataset.txt\",\n",
    "    \"../data/Fuzzy_attack_dataset.txt\",\n",
    "    \"../data/Impersonation_attack_dataset.txt\"\n",
    "]\n",
    "\n",
    "########################################\n",
    "# Patch + Explanation logic\n",
    "########################################\n",
    "def get_patch_and_explanation_for_attack(attack_type, payload):\n",
    "    if attack_type == \"DoS\":\n",
    "        explanation = \"A DoS attack floods the CAN bus with high-priority messages, preventing normal communication.\"\n",
    "        patch = \"Activate the vehicle's security update to handle message overloads.\"\n",
    "    elif attack_type == \"Fuzzy\":\n",
    "        explanation = \"A Fuzzy attack sends malformed or random data to confuse or crash ECUs.\"\n",
    "        patch = \"Enable data validity checks to prevent unsafe reads.\"\n",
    "    elif attack_type == \"Impersonation\":\n",
    "        explanation = \"An Impersonation attack uses spoofed IDs to mimic legitimate ECUs and inject malicious data.\"\n",
    "        patch = \"Turn on ID verification to block unauthorized messages.\"\n",
    "    elif attack_type == \"Attack_free\":\n",
    "        explanation = \"No attack detected. All vehicle systems appear to be operating normally.\"\n",
    "        patch = \"No action needed – vehicle is normal.\"\n",
    "    else:\n",
    "        attack_type = \"Fuzzy\"\n",
    "        explanation = \"A Fuzzy attack sends malformed or random data to confuse or crash ECUs.\"\n",
    "        patch = \"Enable data validity checks to prevent unsafe reads.\"\n",
    "\n",
    "    return f\"Attack Type: {attack_type}\\nExplanation: {explanation}\\nSuggested Patch: {patch}\"\n",
    "\n",
    "########################################\n",
    "# Load & Preprocess CAN data\n",
    "########################################\n",
    "def load_and_preprocess_data(dataset_paths):\n",
    "    data = []\n",
    "\n",
    "    for file_path in dataset_paths:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            continue\n",
    "\n",
    "        attack_type = os.path.basename(file_path).split(\"_\")[0]\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = line.split()\n",
    "            if len(parts) < 7:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                can_id_hex = parts[3]\n",
    "                can_id = int(can_id_hex, 16)\n",
    "                dlc = int(parts[6])\n",
    "                data_bytes = parts[7:7 + dlc]\n",
    "                byte_values = [int(b, 16) for b in data_bytes]\n",
    "\n",
    "                while len(byte_values) < 8:\n",
    "                    byte_values.append(0)\n",
    "\n",
    "                # ✅ Input prompt contains only CAN data\n",
    "                prompt = f\"CAN ID: {can_id}, DLC: {dlc}, Data: {byte_values}\"\n",
    "\n",
    "                # ✅ Target contains attack type, explanation, patch\n",
    "                response = get_patch_and_explanation_for_attack(attack_type, byte_values)\n",
    "\n",
    "                data.append({\"input_text\": prompt, \"target_text\": response})\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load and preprocess data\n",
    "data = load_and_preprocess_data(dataset_files)\n",
    "if not data:\n",
    "    print(\"⚠ No valid lines parsed. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# Convert to Hugging Face dataset\n",
    "hf_dataset = Dataset.from_dict({\n",
    "    \"input_text\": [item[\"input_text\"] for item in data],\n",
    "    \"target_text\": [item[\"target_text\"] for item in data]\n",
    "})\n",
    "\n",
    "# Shuffle and use only the first 5000 samples\n",
    "hf_dataset = hf_dataset.shuffle(seed=42).select(range(min(5000, len(hf_dataset))))\n",
    "\n",
    "########################################\n",
    "# Load tokenizer and model\n",
    "########################################\n",
    "print(\"🔧 Loading DistilGPT2...\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Prevent padding errors\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "########################################\n",
    "# Tokenization\n",
    "########################################\n",
    "def tokenize_function(example):\n",
    "    input_ids = tokenizer(\n",
    "        example[\"input_text\"] + \"\\n\" + example[\"target_text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    return input_ids\n",
    "\n",
    "print(\"🔧 Tokenizing dataset...\")\n",
    "tokenized_dataset = hf_dataset.map(tokenize_function, batched=False, remove_columns=[\"input_text\", \"target_text\"])\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "########################################\n",
    "# Training configuration\n",
    "########################################\n",
    "base_path = \"./\"  # Change this as needed\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(base_path, \"model/distilgpt2_model\"),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    save_steps=500,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=100,\n",
    "    learning_rate=5e-5,\n",
    "    logging_dir=os.path.join(base_path, \"logs\"),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "########################################\n",
    "# Train and Save\n",
    "########################################\n",
    "print(\"🚀 Starting fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "save_path = os.path.join(base_path, \"model/fine_tuned_distilgpt2\")\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"✅ Fine-tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a97a89",
=======
   "execution_count": 2,
   "id": "f178b473",
>>>>>>> c73354a330b159f9adab6290ae89bc89361d717a
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "📥 Input: CAN ID: 450 , DLC: 8, Data: [200, 233, 200, 250, 17, 23, 120, 160]\n",
      "📤 Output:\n",
      " CAN ID: 450 , DLC: 8, Data: [200, 233, 200, 250, 17, 23, 120, 160]\n",
      "Attack Type: Fuzzy\n",
      "Explanation: A Fuzzy attack sends malformed or random data to confuse or crash ECUs.\n",
      "Suggested Patch: Enable data validity checks to prevent unsafe reads.\n",
      "Suggestion No patch suggestion available for Windows XP.\n",
      "Suggestor Code-Friendly Support: Turn on IP verification to block unauthorized messages.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_path = \"./model/fine_tuned_distilgpt2\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Input prompt\n",
    "input_text = \"\"\"CAN ID: 450 , DLC: 8, Data: [200, 233, 200, 250, 17, 23, 120, 160]\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"📥 Input:\", input_text)\n",
    "print(\"📤 Output:\\n\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8df546d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Input: CAN ID: 450 , DLC: 8, Data: [200, 255, 200, 250, 17, 223, 125, 160]\n",
      "📤 Output:\n",
      " CAN ID: 450 , DLC: 8, Data: [200, 255, 200, 250, 17, 223, 125, 160]\n",
      "Attack Type: Fuzzy\n",
      "Explanation: A Fuzzy attack sends malformed or random data to confuse or crash ECUs.\n",
      "Suggested Patch:- Enable data validity checks to prevent unsafe reads.\n"
=======
      "Collecting datasets\n",
      "  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/b4/83/50abe521eb75744a01efe2ebe836a4b61f4df37941a776f650f291aabdf9/datasets-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Obtaining dependency information for pyarrow>=15.0.0 from https://files.pythonhosted.org/packages/ff/77/e62aebd343238863f2c9f080ad2ef6ace25c919c6ab383436b5b81cbeef7/pyarrow-19.0.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Obtaining dependency information for dill<0.3.9,>=0.3.0 from https://files.pythonhosted.org/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl.metadata\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/52/1c/fa3b61c0cf03e1da4767213672efe186b1dfa4fc901a4a694fb184a513d1/xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Obtaining dependency information for multiprocess<0.70.17 from https://files.pythonhosted.org/packages/50/15/b56e50e8debaf439f44befec5b2af11db85f6e0f344c3113ae0be0593a91/multiprocess-0.70.16-py311-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec[http]<=2024.12.0,>=2023.1.0 (from datasets)\n",
      "  Obtaining dependency information for fsspec[http]<=2024.12.0,>=2023.1.0 from https://files.pythonhosted.org/packages/de/86/5486b0188d08aa643e127774a99bac51ffa6cf343e3deb0583956dca5b22/fsspec-2024.12.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/e7/dd/01f6fe028e054ef4f909c9d63e3a2399e77021bb2e1bb51d56ca8b543989/aiohttp-3.11.16-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading aiohttp-3.11.16-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for aiohappyeyeballs>=2.3.0 from https://files.pythonhosted.org/packages/0f/15/5bf3b99495fb160b63f95972b81750f18f7f4e02ad051373b669d17d44f2/aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/ec/6a/bc7e17a3e87a2985d3e8f4da4cd0f481060eb78fb08596c42be62c90a4d9/aiosignal-1.3.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/ca/8c/2ddffeb8b60a4bce3b196c32fcc30d8830d4615e7b492ec2071da801b8ad/frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/85/56/ea976a5e3ebe0e871e004d9cacfe4c803f8ade353eaf4a247580e9dd7b9d/multidict-6.3.2-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading multidict-6.3.2-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for propcache>=0.2.0 from https://files.pythonhosted.org/packages/1d/3a/8a68dd867da9ca2ee9dfd361093e9cb08cb0f37e5ddb2276f1b5177d7731/propcache-0.3.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading propcache-0.3.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.17.0 from https://files.pythonhosted.org/packages/ae/7b/8600250b3d89b625f1121d897062f629883c2f45339623b69b1747ec65fa/yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "     ---------------------------------------- 0.0/71.4 kB ? eta -:--:--\n",
      "     --------------------- ---------------- 41.0/71.4 kB 991.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 71.4/71.4 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\saras\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
      "   ---------------------------------------- 0.0/491.2 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 61.4/491.2 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 122.9/491.2 kB 1.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 194.6/491.2 kB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 286.7/491.2 kB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 358.4/491.2 kB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 440.3/491.2 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 491.2/491.2 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ------------------------ --------------- 71.7/116.3 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 116.3/116.3 kB 1.7 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.11.16-cp311-cp311-win_amd64.whl (442 kB)\n",
      "   ---------------------------------------- 0.0/443.0 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 61.4/443.0 kB 3.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 143.4/443.0 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 225.3/443.0 kB 2.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 307.2/443.0 kB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 389.1/443.0 kB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 443.0/443.0 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.5 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 92.2/143.5 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 143.5/143.5 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading pyarrow-19.0.1-cp311-cp311-win_amd64.whl (25.3 MB)\n",
      "   ---------------------------------------- 0.0/25.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/25.3 MB 3.5 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.3/25.3 MB 4.1 MB/s eta 0:00:07\n",
      "    --------------------------------------- 0.5/25.3 MB 3.5 MB/s eta 0:00:08\n",
      "    --------------------------------------- 0.6/25.3 MB 3.5 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.7/25.3 MB 3.4 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.9/25.3 MB 3.4 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 1.0/25.3 MB 3.3 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 1.2/25.3 MB 3.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.3/25.3 MB 3.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.5/25.3 MB 3.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.6/25.3 MB 3.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.8/25.3 MB 3.3 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 1.9/25.3 MB 3.3 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.1/25.3 MB 3.3 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.2/25.3 MB 3.2 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 2.4/25.3 MB 3.2 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 2.5/25.3 MB 3.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.7/25.3 MB 3.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 2.8/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.0/25.3 MB 3.3 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 3.2/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.3/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.5/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 3.6/25.3 MB 3.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 3.8/25.3 MB 3.3 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 3.9/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.0/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.1/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.3/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 4.4/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 4.5/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 4.7/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 4.8/25.3 MB 3.1 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 5.0/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.1/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.3/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.4/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 5.6/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 5.7/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 5.8/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 6.0/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 6.1/25.3 MB 3.2 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 6.3/25.3 MB 3.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.4/25.3 MB 3.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.5/25.3 MB 3.1 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 6.6/25.3 MB 3.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 6.6/25.3 MB 3.1 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 6.7/25.3 MB 3.0 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 6.8/25.3 MB 3.0 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 6.9/25.3 MB 3.0 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 7.0/25.3 MB 3.0 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 7.0/25.3 MB 2.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 7.1/25.3 MB 2.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 7.2/25.3 MB 2.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 7.2/25.3 MB 2.9 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 7.4/25.3 MB 2.8 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 7.4/25.3 MB 2.8 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 7.5/25.3 MB 2.8 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 7.6/25.3 MB 2.8 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 7.6/25.3 MB 2.8 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 7.7/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 7.8/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 7.9/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 7.9/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 8.0/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 8.1/25.3 MB 2.6 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 8.2/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 8.4/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 8.5/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 8.6/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   ------------- -------------------------- 8.8/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 8.9/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 9.1/25.3 MB 2.7 MB/s eta 0:00:07\n",
      "   -------------- ------------------------- 9.2/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 9.4/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 9.5/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 9.7/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 9.8/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 10.0/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 10.1/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 10.3/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 10.4/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 10.6/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 10.7/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 10.9/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 11.0/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 11.1/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 11.3/25.3 MB 2.7 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 11.5/25.3 MB 2.8 MB/s eta 0:00:06\n",
      "   ------------------ --------------------- 11.6/25.3 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.8/25.3 MB 2.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 11.9/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 12.1/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 12.2/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 12.3/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 12.5/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 12.6/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 12.8/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 12.9/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.1/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 13.3/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.4/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.5/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.7/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.9/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.0/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.1/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.3/25.3 MB 2.7 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.4/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.5/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.7/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.9/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.0/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 15.2/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.3/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.4/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.6/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.8/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.9/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.1/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.1/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.3/25.3 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.4/25.3 MB 2.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 16.6/25.3 MB 2.7 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 16.7/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 16.9/25.3 MB 2.8 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 17.0/25.3 MB 2.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.1/25.3 MB 2.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.3/25.3 MB 2.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.5/25.3 MB 2.9 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.6/25.3 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.8/25.3 MB 3.0 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.9/25.3 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.1/25.3 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.2/25.3 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.4/25.3 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.6/25.3 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.7/25.3 MB 3.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.9/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.1/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.2/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.4/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.5/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.7/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.9/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.0/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.1/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 20.2/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.3/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.5/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.7/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.8/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.0/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.1/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.3/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.4/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.6/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.7/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 21.9/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.0/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.0/25.3 MB 3.2 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.1/25.3 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 22.1/25.3 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 22.2/25.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.4/25.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.5/25.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.7/25.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.7/25.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.8/25.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.0/25.3 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.1/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.2/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.4/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.5/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.7/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.8/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.0/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.2/25.3 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.3/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.4/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.6/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.7/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.0/25.3 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.2/25.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.3/25.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.3/25.3 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "   ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 51.6/51.6 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "   ---------------------------------------- 0.0/183.9 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 61.4/183.9 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 143.4/183.9 kB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 183.9/183.9 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading multidict-6.3.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Downloading propcache-0.3.1-cp311-cp311-win_amd64.whl (45 kB)\n",
      "   ---------------------------------------- 0.0/45.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 45.2/45.2 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "   ---------------------------------------- 0.0/91.0 kB ? eta -:--:--\n",
      "   ------------------------------- -------- 71.7/91.0 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 91.0/91.0 kB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 datasets-3.5.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.3.2 multiprocess-0.70.16 propcache-0.3.1 pyarrow-19.0.1 xxhash-3.5.0 yarl-1.18.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
>>>>>>> c73354a330b159f9adab6290ae89bc89361d717a
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "model_path = \"./model/fine_tuned_distilgpt2\"\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Input prompt\n",
    "input_text = \"\"\"CAN ID: 450 , DLC: 8, Data: [200, 255, 200, 250, 17, 223, 125, 160]\"\"\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    temperature=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"📥 Input:\", input_text)\n",
    "print(\"📤 Output:\\n\", output_text)"
=======
    "pip install datasets"
>>>>>>> c73354a330b159f9adab6290ae89bc89361d717a
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.13.2"
=======
   "version": "3.11.6"
>>>>>>> c73354a330b159f9adab6290ae89bc89361d717a
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
